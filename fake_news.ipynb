{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "066706b0-d4ee-4788-b742-3579751dc425",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-21 14:38:16.700367: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-01-21 14:38:16.742424: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-01-21 14:38:17.344991: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2024-01-21 14:38:38.097783: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-01-21 14:38:38.109708: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1960] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "632/632 [==============================] - 103s 160ms/step - loss: 0.1078 - accuracy: 0.9633 - val_loss: 0.1793 - val_accuracy: 0.9423\n",
      "Epoch 2/5\n",
      "632/632 [==============================] - 102s 161ms/step - loss: 0.0396 - accuracy: 0.9888 - val_loss: 0.3778 - val_accuracy: 0.8987\n",
      "Epoch 3/5\n",
      "632/632 [==============================] - 106s 167ms/step - loss: 0.0355 - accuracy: 0.9897 - val_loss: 0.4581 - val_accuracy: 0.9038\n",
      "Epoch 4/5\n",
      "632/632 [==============================] - 106s 167ms/step - loss: 0.0377 - accuracy: 0.9886 - val_loss: 0.3707 - val_accuracy: 0.8931\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Veri setinizi yükleyin (varsayılan olarak 'doğru_haberler' ve 'yanlis_haberler' olarak adlandırıldı)\n",
    "doğru_haberler = pd.read_csv(\"true.csv\")\n",
    "yanlis_haberler = pd.read_csv(\"fake.csv\")\n",
    "\n",
    "# Veri önişleme\n",
    "# Metin verilerini ve etiketleri birleştirin (doğru = 1, yanlış = 0)\n",
    "metinler = np.concatenate([doğru_haberler['text'], yanlis_haberler['text']])\n",
    "etiketler = np.concatenate([np.ones(len(doğru_haberler)), np.zeros(len(yanlis_haberler))])\n",
    "\n",
    "# Tokenizasyon\n",
    "tokenizer = Tokenizer(num_words=5000)\n",
    "tokenizer.fit_on_texts(metinler)\n",
    "sequences = tokenizer.texts_to_sequences(metinler)\n",
    "data = pad_sequences(sequences, maxlen=200)\n",
    "\n",
    "# Model oluşturma\n",
    "model = Sequential()\n",
    "model.add(Embedding(5000, 100, input_length=200))\n",
    "model.add(SpatialDropout1D(0.2))\n",
    "model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Define the early stopping callback\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',     # Monitor the validation loss\n",
    "    patience=3,             # Number of epochs with no improvement after which training will be stopped\n",
    "    restore_best_weights=True  # Restore model weights from the epoch with the best value of the monitored quantity\n",
    ")\n",
    "\n",
    "# Fit the model with the early stopping callback\n",
    "history = model.fit(\n",
    "    data,\n",
    "    etiketler,\n",
    "    epochs=5,\n",
    "    batch_size=64,\n",
    "    validation_split=0.1,\n",
    "    callbacks=[early_stopping]  # Add the callback to the training process\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "402d5f87-23da-4570-a4bf-6f3d20aaad80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSV dosyasını okuyun\n",
    "fake_news_df = pd.read_csv(\"fake.csv\")\n",
    "\n",
    "# Gerekirse, veri temizleme veya ön işleme yapın\n",
    "# Örneğin, boş metinleri kaldırabilir veya tüm metinleri küçük harfe dönüştürebilirsiniz\n",
    "\n",
    "# 'title', 'text', 'subject', 'date' sütunlarını birleştirin\n",
    "# Her bir haber için birleştirilmiş metin dizesi oluşturun\n",
    "# Bunun için daha karmaşık veya basit birleştirmeler yapabilirsiniz, burada basit bir örnek verilmiştir\n",
    "fake_news_df['combined_text'] = fake_news_df['text'] + \" \" + fake_news_df['subject']\n",
    "\n",
    "# Birleştirilmiş metinleri bir metin dosyasına yazın\n",
    "with open('sahte_haberler.txt', 'w', encoding='utf-8') as f:\n",
    "    for text in fake_news_df['combined_text']:\n",
    "        f.write(text + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe42d63-fb62-4c40-bc01-979a5e94e2ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, TextDataset, DataCollatorForLanguageModeling, Trainer, TrainingArguments\n",
    "\n",
    "# Model ve tokenizer yükleyin\n",
    "model_name = 'gpt2'\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "\n",
    "# Sahte haber veri setinizi yükleyin ve bir TextDataset oluşturun\n",
    "train_dataset = TextDataset(\n",
    "    tokenizer=tokenizer,\n",
    "    file_path=\"sahte_haberler.txt\",  # Sahte haber veri setinizin yolu\n",
    "    block_size=128\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=False\n",
    ")\n",
    "\n",
    "# Eğitim ayarlarını yapılandırın\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./gpt2-finetuned\",  # Model çıktılarının ve checkpoint'lerin kaydedileceği dizin\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=4,\n",
    "    save_steps=10_000,\n",
    "    save_total_limit=2,\n",
    ")\n",
    "\n",
    "# Trainer oluşturun ve eğitimi başlatın\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "# Eğitilen modeli kullanarak sahte haber metni üretin\n",
    "prompt = \"Write false news about this subject: economy\"\n",
    "\n",
    "inputs = tokenizer.encode(prompt, return_tensors='pt')\n",
    "outputs = model.generate(inputs, max_length=200, num_return_sequences=1)\n",
    "text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a0f760-dc62-4cce-b973-79c544ddb5d7",
   "metadata": {},
   "source": [
    "Evet, verdiğiniz kod parçası, bir GPT-2 modelinin ince ayar (fine-tuning) işlemini gerçekleştirmek için kullanılır. Bu işlem sırasında, önceden eğitilmiş bir model (GPT2LMHeadModel.from_pretrained('gpt2')) alınır ve belirli bir veri seti üzerinde daha fazla eğitilerek spesifik bir göreve uyarlanır. Bu durumda, model sahte haber metinlerini içeren sahte_haberler.txt dosyası kullanılarak ince ayar yapılıyor.\n",
    "\n",
    "İşte her adımda ne yapıldığını açıklayan bir özet:\n",
    "\n",
    "Model ve Tokenizer Yükleme: GPT-2 modeli ve tokenizer'ı transformers kütüphanesi kullanılarak yüklenir. Tokenizer, metinleri modele verilecek token dizilerine çevirir.\n",
    "\n",
    "TextDataset Oluşturma: TextDataset, eğitim için kullanılacak veri setini yükler ve modelin eğitileceği bloklara ayırır.\n",
    "\n",
    "DataCollator Oluşturma: DataCollatorForLanguageModeling, token dizilerini toplu işlem haline getirir ve gerekirse padding yapar.\n",
    "\n",
    "Eğitim Argümanlarını Yapılandırma: TrainingArguments ile eğitim parametreleri belirlenir, örneğin eğitim süresi, batch boyutu ve checkpoint kaydetme ayarları.\n",
    "\n",
    "Trainer Oluşturma ve Eğitim: Trainer sınıfı, model eğitim sürecini yönetir ve train() metodu ile eğitimi başlatır.\n",
    "\n",
    "Metin Üretme: Eğitim tamamlandıktan sonra, model bir prompt alır ve yeni metin dizileri üretir.\n",
    "\n",
    "Verilen kodda trainer.train() komutu ile modelin eğitimi başlatılır ve bu işlem modeli ince ayarlamak anlamına gelir. Ancak, kodu çalıştırmadan önce sahte_haberler.txt dosyasının doğru formatlandığından ve ilgili veri setini içerdiğinden emin olmalısınız. Bu kod parçasının başarıyla çalışabilmesi için dosyanın ve ortamın düzgün şekilde hazırlanmış olması gerekmektedir.\n",
    "\n",
    "Ayrıca, bu kodun çalıştırılması GPU veya TPU gibi yüksek işlem gücüne sahip bir makine gerektirebilir, çünkü GPT-2 gibi büyük modellerin eğitimi ciddi miktarda hesaplama kaynağı tüketir. Eğer hali hazırda bir Hugging Face hesabınız varsa ve uygun API token'ınıza sahipseniz, bu işlemi Hugging Face'in sunucularında gerçekleştirebilirsiniz.\n",
    "\n",
    "Eğitim sürecinin tamamlanmasının ardından, modeliniz özel veri setinize göre ince ayarlanmış olacak ve bu veri setinin stilinde metin üretebilecektir.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
